{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c07fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "# suppress runtime warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "378f3aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "BATCH_SIZE = 256\n",
    "MODEL_DIR = 'models'\n",
    "SAVE_FIGS = True     # whether to save figures folder\n",
    "\n",
    "# make models directory\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "model_save_path = os.path.join(MODEL_DIR, 'mlp_best_model.pth')\n",
    "\n",
    "# seed\n",
    "np.random.seed(2025)\n",
    "SHAP_waterfall_ct = 3 # 3 random samples\n",
    "\n",
    "# make figures and shap directories\n",
    "if SAVE_FIGS and not os.path.exists('figures'):\n",
    "    os.makedirs('figures')\n",
    "if SAVE_FIGS and not os.path.exists('figures/shap'):\n",
    "    os.makedirs('figures/shap')\n",
    "if SAVE_FIGS and not os.path.exists('figures/lime'):\n",
    "    os.makedirs('figures/lime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9e7ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "pok\n",
      "0    0.78354\n",
      "1    0.21646\n",
      "Name: proportion, dtype: float64\n",
      "Training samples: 1016881, Validation samples: 65776, Test samples: 108489\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>thetao</th>\n",
       "      <th>uo</th>\n",
       "      <th>vo</th>\n",
       "      <th>so</th>\n",
       "      <th>thetao_grad</th>\n",
       "      <th>chl</th>\n",
       "      <th>no3</th>\n",
       "      <th>nppv</th>\n",
       "      <th>o2</th>\n",
       "      <th>po4</th>\n",
       "      <th>si</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>day_sin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.9758</td>\n",
       "      <td>-24.0175</td>\n",
       "      <td>246.625200</td>\n",
       "      <td>5.228278</td>\n",
       "      <td>-0.043947</td>\n",
       "      <td>0.062258</td>\n",
       "      <td>35.019989</td>\n",
       "      <td>3.897163</td>\n",
       "      <td>0.069659</td>\n",
       "      <td>7.836014</td>\n",
       "      <td>0.017190</td>\n",
       "      <td>287.540741</td>\n",
       "      <td>0.580970</td>\n",
       "      <td>3.890184</td>\n",
       "      <td>0.999408</td>\n",
       "      <td>0.034398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64.9185</td>\n",
       "      <td>-24.0353</td>\n",
       "      <td>108.751520</td>\n",
       "      <td>5.502213</td>\n",
       "      <td>0.065310</td>\n",
       "      <td>0.211188</td>\n",
       "      <td>35.065769</td>\n",
       "      <td>6.025003</td>\n",
       "      <td>0.069659</td>\n",
       "      <td>7.836014</td>\n",
       "      <td>0.017190</td>\n",
       "      <td>287.540741</td>\n",
       "      <td>0.580970</td>\n",
       "      <td>3.890184</td>\n",
       "      <td>0.999408</td>\n",
       "      <td>0.034398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64.8673</td>\n",
       "      <td>-24.0747</td>\n",
       "      <td>64.917168</td>\n",
       "      <td>5.178473</td>\n",
       "      <td>-0.001831</td>\n",
       "      <td>0.319834</td>\n",
       "      <td>35.027618</td>\n",
       "      <td>4.765457</td>\n",
       "      <td>0.081872</td>\n",
       "      <td>6.923312</td>\n",
       "      <td>0.052279</td>\n",
       "      <td>292.967468</td>\n",
       "      <td>0.527286</td>\n",
       "      <td>3.415034</td>\n",
       "      <td>0.999408</td>\n",
       "      <td>0.034398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64.9198</td>\n",
       "      <td>-24.0180</td>\n",
       "      <td>80.465360</td>\n",
       "      <td>5.502213</td>\n",
       "      <td>0.065310</td>\n",
       "      <td>0.211188</td>\n",
       "      <td>35.065769</td>\n",
       "      <td>6.025003</td>\n",
       "      <td>0.069659</td>\n",
       "      <td>7.836014</td>\n",
       "      <td>0.017190</td>\n",
       "      <td>287.540741</td>\n",
       "      <td>0.580970</td>\n",
       "      <td>3.890184</td>\n",
       "      <td>0.999408</td>\n",
       "      <td>0.034398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63.5857</td>\n",
       "      <td>-20.4557</td>\n",
       "      <td>41.604976</td>\n",
       "      <td>6.356242</td>\n",
       "      <td>0.012207</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>34.681232</td>\n",
       "      <td>3.200496</td>\n",
       "      <td>0.093789</td>\n",
       "      <td>5.887146</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>286.737061</td>\n",
       "      <td>0.464209</td>\n",
       "      <td>3.390041</td>\n",
       "      <td>0.999408</td>\n",
       "      <td>0.034398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   latitude  longitude       depth    thetao        uo        vo         so  \\\n",
       "0   64.9758   -24.0175  246.625200  5.228278 -0.043947  0.062258  35.019989   \n",
       "1   64.9185   -24.0353  108.751520  5.502213  0.065310  0.211188  35.065769   \n",
       "2   64.8673   -24.0747   64.917168  5.178473 -0.001831  0.319834  35.027618   \n",
       "3   64.9198   -24.0180   80.465360  5.502213  0.065310  0.211188  35.065769   \n",
       "4   63.5857   -20.4557   41.604976  6.356242  0.012207  0.001221  34.681232   \n",
       "\n",
       "   thetao_grad       chl       no3      nppv          o2       po4        si  \\\n",
       "0     3.897163  0.069659  7.836014  0.017190  287.540741  0.580970  3.890184   \n",
       "1     6.025003  0.069659  7.836014  0.017190  287.540741  0.580970  3.890184   \n",
       "2     4.765457  0.081872  6.923312  0.052279  292.967468  0.527286  3.415034   \n",
       "3     6.025003  0.069659  7.836014  0.017190  287.540741  0.580970  3.890184   \n",
       "4     3.200496  0.093789  5.887146  0.062200  286.737061  0.464209  3.390041   \n",
       "\n",
       "    day_cos   day_sin  \n",
       "0  0.999408  0.034398  \n",
       "1  0.999408  0.034398  \n",
       "2  0.999408  0.034398  \n",
       "3  0.999408  0.034398  \n",
       "4  0.999408  0.034398  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/pok_2010_2025_augmented.csv')\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# make pok as ratio greater than 0.1\n",
    "data['pok'] = (data['pok_ratio'] > 0.1).astype(int)\n",
    "print(f\"Class distribution:\\n{data['pok'].value_counts(normalize=True)}\")\n",
    "\n",
    "val_splitday = pd.to_datetime('2022-12-31')\n",
    "test_splitday = pd.to_datetime('2023-12-31')\n",
    "\n",
    "# y-ratio vector associated with X_test\n",
    "y_test_ratio = data[data['date'] > test_splitday]['pok_ratio'].reset_index(drop=True)\n",
    "\n",
    "# prepare data\n",
    "data = data.drop(columns=['species', 'gear', 'weight', 'pok_ratio'])\n",
    "\n",
    "data_train = data[data['date'] <= val_splitday]\n",
    "data_val = data[(data['date'] > val_splitday) & (data['date'] <= test_splitday)]\n",
    "data_test = data[data['date'] > test_splitday]\n",
    "\n",
    "X_train = data_train.drop(columns=['pok', 'date']).reset_index(drop=True)\n",
    "y_train = data_train['pok'].reset_index(drop=True)\n",
    "X_val = data_val.drop(columns=['pok', 'date']).reset_index(drop=True)\n",
    "y_val = data_val['pok'].reset_index(drop=True)\n",
    "X_test = data_test.drop(columns=['pok', 'date']).reset_index(drop=True)\n",
    "y_test = data_test['pok'].reset_index(drop=True)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}, Test samples: {len(X_test)}\")\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7996cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)  # only transform\n",
    "X_test_scaled = scaler.transform(X_test) # only transform\n",
    "\n",
    "# # make dataframes again (for feature selection later)\n",
    "# X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "# X_val_df   = pd.DataFrame(X_val_scaled,   columns=X_val.columns,   index=X_val.index)\n",
    "# X_test_df  = pd.DataFrame(X_test_scaled,  columns=X_test.columns,  index=X_test.index)\n",
    "\n",
    "# make tensors\n",
    "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_val_t = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "y_val_t = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# make dataset\n",
    "trainset = TensorDataset(X_train_t, y_train_t)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# make val dataset\n",
    "valset = TensorDataset(X_val_t, y_val_t)\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd7a5608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression training accuracy: 0.7956\n",
      "Logistic Regression test accuracy: 0.8001\n",
      "Logistic Regression test ROC AUC: 0.8244\n"
     ]
    }
   ],
   "source": [
    "# fit logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train_scaled, y_train) \n",
    "print(f\"Logistic Regression training accuracy: {logreg.score(X_train_scaled, y_train):.4f}\")\n",
    "print(f\"Logistic Regression test accuracy: {logreg.score(X_test_scaled, y_test):.4f}\")\n",
    "\n",
    "# roc auc\n",
    "y_test_probs_logreg = logreg.predict_proba(X_test_scaled)[:, 1]\n",
    "test_roc_auc_logreg = roc_auc_score(y_test, y_test_probs_logreg)\n",
    "print(f\"Logistic Regression test ROC AUC: {test_roc_auc_logreg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fc7ad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest training accuracy: 1.0000\n",
      "Random Forest test accuracy: 0.8607\n",
      "Random Forest test ROC AUC: 0.9075\n"
     ]
    }
   ],
   "source": [
    "# random forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=2025)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "print(f\"Random Forest training accuracy: {rf_model.score(X_train_scaled, y_train):.4f}\")\n",
    "print(f\"Random Forest test accuracy: {rf_model.score(X_test_scaled, y_test):.4f}\")\n",
    "\n",
    "# roc auc\n",
    "y_test_probs_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "test_roc_auc_rf = roc_auc_score(y_test, y_test_probs_rf)\n",
    "print(f\"Random Forest test ROC AUC: {test_roc_auc_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4831959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost training accuracy: 0.8850\n",
      "XGBoost test accuracy: 0.8613\n",
      "XGBoost test ROC AUC: 0.9077\n"
     ]
    }
   ],
   "source": [
    "# XGBoost model\n",
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=2025)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "print(f\"XGBoost training accuracy: {xgb_model.score(X_train_scaled, y_train):.4f}\")\n",
    "print(f\"XGBoost test accuracy: {xgb_model.score(X_test_scaled, y_test):.4f}\")\n",
    "# roc auc\n",
    "y_test_probs_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "test_roc_auc_xgb = roc_auc_score(y_test, y_test_probs_xgb)\n",
    "print(f\"XGBoost test ROC AUC: {test_roc_auc_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dddc7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN training accuracy: 0.9133\n",
      "KNN test accuracy: 0.8272\n",
      "KNN test ROC AUC: 0.8280\n"
     ]
    }
   ],
   "source": [
    "# KNN model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "print(f\"KNN training accuracy: {knn_model.score(X_train_scaled, y_train):.4f}\")\n",
    "print(f\"KNN test accuracy: {knn_model.score(X_test_scaled, y_test):.4f}\")\n",
    "# roc auc\n",
    "y_test_probs_knn = knn_model.predict_proba(X_test_scaled)[:, 1]\n",
    "test_roc_auc_knn = roc_auc_score(y_test, y_test_probs_knn)\n",
    "print(f\"KNN test ROC AUC: {test_roc_auc_knn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e1677a",
   "metadata": {},
   "source": [
    "# NN training (with adding features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db25779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PollockClassifier(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a09e577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: ['latitude']\n",
      "Early stopping triggered at epoch 13 after 5 epochs without improvement.\n",
      "Best Val Loss: 0.4256\n",
      "#Features=1 (['latitude']): Accuracy=0.801, AUC=0.751\n",
      "Using: ['latitude', 'longitude']\n",
      "Early stopping triggered at epoch 21 after 5 epochs without improvement.\n",
      "Best Val Loss: 0.3312\n",
      "#Features=2 (['latitude', 'longitude']): Accuracy=0.840, AUC=0.886\n",
      "Using: ['latitude', 'longitude', 'depth']\n",
      "Early stopping triggered at epoch 12 after 5 epochs without improvement.\n",
      "Best Val Loss: 0.3124\n",
      "#Features=3 (['latitude', 'longitude', 'depth']): Accuracy=0.846, AUC=0.892\n",
      "Using: ['latitude', 'longitude', 'depth', 'po4']\n",
      "Early stopping triggered at epoch 29 after 5 epochs without improvement.\n",
      "Best Val Loss: 0.3089\n",
      "#Features=4 (['latitude', 'longitude', 'depth', 'po4']): Accuracy=0.845, AUC=0.895\n",
      "Using: ['latitude', 'longitude', 'depth', 'po4', 'no3']\n",
      "Early stopping triggered at epoch 33 after 5 epochs without improvement.\n",
      "Best Val Loss: 0.3057\n",
      "#Features=5 (['latitude', 'longitude', 'depth', 'po4', 'no3']): Accuracy=0.852, AUC=0.896\n",
      "Using: ['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao']\n",
      "Early stopping triggered at epoch 9 after 5 epochs without improvement.\n",
      "Best Val Loss: 0.3115\n",
      "#Features=6 (['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao']): Accuracy=0.843, AUC=0.889\n",
      "Using: ['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao', 'day_cos', 'day_sin']\n",
      "Early stopping triggered at epoch 24 after 5 epochs without improvement.\n",
      "Best Val Loss: 0.2997\n",
      "#Features=8 (['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao', 'day_cos', 'day_sin']): Accuracy=0.860, AUC=0.905\n",
      "Using: ['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao', 'day_cos', 'day_sin', 'o2']\n",
      "Early stopping triggered at epoch 20 after 5 epochs without improvement.\n",
      "Best Val Loss: 0.3005\n",
      "#Features=9 (['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao', 'day_cos', 'day_sin', 'o2']): Accuracy=0.854, AUC=0.898\n",
      "Using: ['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao', 'day_cos', 'day_sin', 'o2', 'so']\n",
      "Early stopping triggered at epoch 12 after 5 epochs without improvement.\n",
      "Best Val Loss: 0.3064\n",
      "#Features=10 (['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao', 'day_cos', 'day_sin', 'o2', 'so']): Accuracy=0.857, AUC=0.898\n",
      "Using: ['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao', 'day_cos', 'day_sin', 'o2', 'so', 'si']\n",
      "Early stopping triggered at epoch 15 after 5 epochs without improvement.\n",
      "Best Val Loss: 0.3031\n",
      "#Features=11 (['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao', 'day_cos', 'day_sin', 'o2', 'so', 'si']): Accuracy=0.853, AUC=0.897\n",
      "Using: ['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao', 'day_cos', 'day_sin', 'o2', 'so', 'si', 'chl']\n",
      "Early stopping triggered at epoch 21 after 5 epochs without improvement.\n",
      "Best Val Loss: 0.3058\n",
      "#Features=12 (['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao', 'day_cos', 'day_sin', 'o2', 'so', 'si', 'chl']): Accuracy=0.852, AUC=0.896\n",
      "Using: ['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao', 'day_cos', 'day_sin', 'o2', 'so', 'si', 'chl', 'vo', 'uo']\n",
      "Early stopping triggered at epoch 14 after 5 epochs without improvement.\n",
      "Best Val Loss: 0.3061\n",
      "#Features=14 (['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao', 'day_cos', 'day_sin', 'o2', 'so', 'si', 'chl', 'vo', 'uo']): Accuracy=0.855, AUC=0.898\n",
      "Using: ['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao', 'day_cos', 'day_sin', 'o2', 'so', 'si', 'chl', 'vo', 'uo', 'thetao_grad']\n",
      "Early stopping triggered at epoch 23 after 5 epochs without improvement.\n",
      "Best Val Loss: 0.3037\n",
      "#Features=15 (['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao', 'day_cos', 'day_sin', 'o2', 'so', 'si', 'chl', 'vo', 'uo', 'thetao_grad']): Accuracy=0.853, AUC=0.898\n",
      "Using: ['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao', 'day_cos', 'day_sin', 'o2', 'so', 'si', 'chl', 'vo', 'uo', 'thetao_grad', 'nppv']\n",
      "Early stopping triggered at epoch 16 after 5 epochs without improvement.\n",
      "Best Val Loss: 0.3059\n",
      "#Features=16 (['latitude', 'longitude', 'depth', 'po4', 'no3', 'thetao', 'day_cos', 'day_sin', 'o2', 'so', 'si', 'chl', 'vo', 'uo', 'thetao_grad', 'nppv']): Accuracy=0.853, AUC=0.897\n"
     ]
    }
   ],
   "source": [
    "results = []   # to store results: (num_features, used_feature_names, accuracy, auc)\n",
    "\n",
    "selected_features = []  # add features step by step\n",
    "feature_steps = [\n",
    "    ['latitude'],\n",
    "    ['longitude'],\n",
    "    ['depth'],\n",
    "    ['po4'],\n",
    "    ['no3'],\n",
    "    ['thetao'],\n",
    "    ['day_cos', 'day_sin'],  \n",
    "    ['o2'],\n",
    "    ['so'],\n",
    "    ['si'],\n",
    "    ['chl'],\n",
    "    ['vo', 'uo'],\n",
    "    ['thetao_grad'],\n",
    "    ['nppv']\n",
    "]\n",
    "\n",
    "\n",
    "for step_features in feature_steps:\n",
    "    selected_features.extend(step_features)\n",
    "    print(\"Using:\", selected_features)\n",
    "\n",
    "    # subset data to selected features\n",
    "    X_train_subset = X_train[selected_features]\n",
    "    X_val_subset = X_val[selected_features]\n",
    "    X_test_subset = X_test[selected_features]\n",
    "\n",
    "    # scale the data\n",
    "    subscaler = StandardScaler()\n",
    "    X_train_k_scaled = subscaler.fit_transform(X_train_subset)\n",
    "    X_val_k_scaled = subscaler.transform(X_val_subset)\n",
    "    X_test_k_scaled = subscaler.transform(X_test_subset)\n",
    "\n",
    "    X_train_k = torch.tensor(X_train_k_scaled, dtype=torch.float32)\n",
    "    X_val_k   = torch.tensor(X_val_k_scaled,   dtype=torch.float32)\n",
    "    X_test_k  = torch.tensor(X_test_k_scaled,  dtype=torch.float32)\n",
    "    trainloader = DataLoader(TensorDataset(X_train_k, y_train_t), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valloader   = DataLoader(TensorDataset(X_val_k,   y_val_t),   batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # new model for current feature set\n",
    "    model = PollockClassifier(in_features=len(selected_features))\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "    # early stopping parameters\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 5\n",
    "    counter = 0\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = loss_function(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        avg_train_loss = total_loss / len(trainloader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in valloader:\n",
    "                val_preds = model(xb)\n",
    "                loss = loss_function(val_preds, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                val_preds = (val_preds > 0.5).float()\n",
    "                correct += (val_preds == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(valloader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        val_accuracy = correct / total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        #print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "        # early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered at epoch\", epoch+1, \"after\", patience, \"epochs without improvement.\")\n",
    "            print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    # load best model state\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    # evaluate on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_probs = model(X_test_k).numpy()\n",
    "\n",
    "    acc = accuracy_score(y_test_t.numpy(), test_probs > 0.5)\n",
    "    auc = roc_auc_score(y_test_t.numpy(), test_probs)\n",
    "\n",
    "    print(f\"#Features={len(selected_features)} ({selected_features}): Accuracy={acc:.3f}, AUC={auc:.3f}\")\n",
    "\n",
    "    results.append((len(selected_features), step_features, acc, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be15572b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ['latitude'], 0.8005143378591378, 0.7509501352118525),\n",
       " (2, ['longitude'], 0.840426218326282, 0.8858497173944955),\n",
       " (3, ['depth'], 0.8456249020638037, 0.8920457040200631),\n",
       " (4, ['po4'], 0.8446847145793582, 0.8947035744786854),\n",
       " (5, ['no3'], 0.8515978578473393, 0.8957560120649195),\n",
       " (6, ['thetao'], 0.8427490344643236, 0.8885551955803208),\n",
       " (8, ['day_cos', 'day_sin'], 0.8602070255970652, 0.9045062950315929),\n",
       " (9, ['o2'], 0.8541326770455991, 0.8976264347843794),\n",
       " (10, ['so'], 0.8565107983297846, 0.8976249760802103),\n",
       " (11, ['si'], 0.8527684834407175, 0.8973505134557055),\n",
       " (12, ['chl'], 0.851975776345989, 0.8963875746012786),\n",
       " (14, ['vo', 'uo'], 0.8545105955442487, 0.8984997593635493),\n",
       " (15, ['thetao_grad'], 0.8529804865009356, 0.897780443745205),\n",
       " (16, ['nppv'], 0.8532293596585829, 0.8967922427056016)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
